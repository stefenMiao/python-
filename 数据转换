----------移除重复数据-----------
df
    k1  k2
0  one   1
1  two   1
2  one   2
3  two   3
4  one   3
5  two   4
6  two   4

duplicated方法
判断该行是否重复（前面是否出现过）
df.duplicated()
0    False
1    False
2    False
3    False
4    False
5    False
6     True

drop_duplicates方法
返回移除重复行的dataframe
df.drop_duplicates()
    k1  k2
0  one   1
1  two   1
2  one   2
3  two   3
4  one   3
5  two   4

这两种方法默认判断全部列，可以指定部分列进行判断
df
    k1  k2  v1
0  one   1   0
1  two   1   1
2  one   2   2
3  two   3   3
4  one   3   4
5  two   4   5
6  two   4   6

df.drop_duplicates(['k1'])   #只判断k1列是否有重复行 
    k1  k2  v1
0  one   1   0
1  two   1   1

keep参数
当出现重复时默认保留第一个数据，keep='last'则保留最后一个数据
df.drop_duplicates(['k1','k2'],keep='last')
    k1  k2  v1
0  one   1   0
1  two   1   1
2  one   2   2
3  two   3   3
4  one   3   4
6  two   4   6  #保留了第六行而不是第五行


----------利用函数或映射进行数据转换------------
data
          food  ounces
0        bacon     4.0
1  pulled pork     3.0
2        bacon    12.0
3     Pastrami     6.0
4  corned beef     7.5
5        Bacon     8.0
6     pastrami     3.0
7    honey ham     5.0
8     nova lox     6.0
要在data的基础上加入下列字典
meat_to_animal = {
'bacon': 'pig',
'pulled pork': 'pig',
'pastrami': 'cow',
'corned beef': 'cow',
'honey ham': 'pig',
'nova lox': 'salmon'
}
由于food数据存在大小写杂糅，所以把它们全都转换为小写
low=data['food'].str.lower()

使用map方法组合字典与原数据
data['animal']=low.map(meat_to_animal)
          food  ounces  animal
0        bacon     4.0     pig
1  pulled pork     3.0     pig
2        bacon    12.0     pig
3     Pastrami     6.0     cow
4  corned beef     7.5     cow
5        Bacon     8.0     pig
6     pastrami     3.0     cow
7    honey ham     5.0     pig
8     nova lox     6.0  salmon

使用map是一种实现元素级转换以及其他数据清理工作的便捷方式


------------替换值-------------
replace方法
进行值替换，产生新的副本
data
0      10
1      20
2   -1000
3    -999
4       3
5    -999

data.replace(-999,np.nan)
0      10.0
1      20.0
2   -1000.0
3       NaN
4       3.0
5       NaN

data.replace([-999,-1000],[np.nan,0])  #一次进行多个值替换，传入列表
0    10.0
1    20.0
2     0.0
3     NaN
4     3.0
5     NaN

data.replace({-999:np.nan,-1000:0})  #传入字典进行值替换
0    10.0
1    20.0
2     0.0
3     NaN
4     3.0
5     NaN


---------重命名轴索引---------
轴标签也可以通过函数或映射转换
data
          one  two  three  four
Ohio        0    1      2     3
Colorado    4    5      6     7
New York    8    9     10    11

trans=lambda x:x[0:4].upper()

data.index=data.index.map(trans)
      one  two  three  four
OHIO    0    1      2     3
COLO    4    5      6     7
NEW     8    9     10    11


rename方法
创建数据集的转换版，而不是修改原始数据
data.rename(index=str.title,columns=str.upper)
      ONE  TWO  THREE  FOUR
Ohio    0    1      2     3
Colo    4    5      6     7
New     8    9     10    11

可以结合字典，对部分轴标签更改
data.rename(index={'COLO':'old'},columns={'two':'five'})
      one  five  three  four
OHIO    0     1      2     3
old     4     5      6     7
NEW     8     9     10    11

就地修改则传入inplace=True


----------离散化和面元划分---------
为了便于分析，连续的数据常常被离散化或者划分为多个面元bin（部分）

cut函数
pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates='raise') 
x：被切分的类数组（array-like）数据，必须是1维的（不能用DataFrame）；
bins：bins是被切割后的区间（或者叫“桶”、“箱”、“面元”），有3中形式：一个int型的标量、标量序列（数组）或者pandas.IntervalIndex 。
    一个int型的标量
    当bins为一个int型的标量时，代表将x平分成bins份。x的范围在每侧扩展0.1%，以包括x的最大值和最小值。
    标量序列
    标量序列定义了被分割后每一个bin的区间边缘，此时x没有扩展。
    pandas.IntervalIndex
    定义要使用的精确区间。
right：bool型参数，默认为True，表示是否包含区间右部。比如如果bins=[1,2,3]，right=True，则区间为(1,2]，(2,3]；right=False，则区间为(1,2),(2,3)。
labels：给分割后的bins打标签，比如把年龄x分割成年龄段bins后，可以给年龄段打上诸如青年、中年的标签。labels的长度必须和划分后的区间长度相等，比如bins=[1,2,3]，划分后有2个区间(1,2]，(2,3]，则labels的长度必须为2。如果指定labels=False，则返回x中的数据在第几个bin中（从0开始）。
retbins：bool型的参数，表示是否将分割后的bins返回，当bins为一个int型的标量时比较有用，这样可以得到划分后的区间，默认为False。
precision：保留区间小数点的位数，默认为3.
include_lowest：bool型的参数，表示区间的左边是开还是闭的，默认为false，也就是不包含区间左部（闭）。
duplicates：是否允许重复区间。有两种选择：raise：不允许，drop：允许。

ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
bins=[18,25,35,60,100]   #进行划分的区间

cats=pd.cut(ages,bins)

cats 
[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]  #每个数据所处的区间
Length: 12  #数据长度
Categories (4, interval[int64]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]  #划分区间 

cats.categories
IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]],  #right=True左开右闭
              closed='right',
              dtype='interval[int64]')

cats.codes
array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)

传入标签
cats=pd.cut(ages,bins,labels=['child','young','adult','old'])
['child', 'child', 'child', 'young', 'child', ..., 'young', 'old', 'adult', 'adult', 'young']  #传入标签后，使用标签表示
Length: 12
Categories (4, object): ['child' < 'young' < 'adult' < 'old']

data=np.random.randn(20)

pd.cut(data,4,precision=2)  #直接传入数字n表示把数据分为n等份，precision表示小数点精度为两位
[(-1.47, -0.099], (1.27, 2.65], (-1.47, -0.099], (-0.099, 1.27], (-2.85, -1.47], ..., (-1.47, -0.099], (-1.47, -0.099], (-0.099, 1.27], (-1.47, -0.099], (-1.47, -0.099]]
Length: 20
Categories (4, interval[float64]): [(-2.85, -1.47] < (-1.47, -0.099] < (-0.099, 1.27] < (1.27, 2.65]]


qcut函数
可以根据样本分位数对数据进行面元划分。根据数据的分布情况，cut可能无法使各个面元中含有相同数量的数据点。而qcut由于使用的是样本分位数，因此可以得到大小基本相等的面元

pd.qcut(data,4)
[(0.693, 2.915], (-2.48, -0.6], (0.693, 2.915], (-0.6, 0.0999], (0.693, 2.915], ..., (-2.48, -0.6], (-0.6, 0.0999], (-2.48, -0.6], (0.0999, 0.693], (0.0999, 0.693]]
Length: 200
Categories (4, interval[float64]): [(-2.48, -0.6] < (-0.6, 0.0999] < (0.0999, 0.693] <(0.693, 2.915]]

pd.value_counts(cats)  #按照数据个数平分
(-2.48, -0.6]      50
(-0.6, 0.0999]     50
(0.0999, 0.693]    50
(0.693, 2.915]     50


------------检测和过滤异常值-------------
过滤或变换异常值常使用数组运算
data=pd.DataFrame(np.random.randn(1000,4))

找出某个列中绝对值大于3的值
col=data[2]
col[np.abs(col)>3]
308    3.038356
420    3.719493
540   -3.312577
908    3.013013

选出全部含有“超过3或－3的值”的行
data[(np.abs(data)>3).any(1)]
            0         1         2         3
206  0.060288 -0.760483  0.427357  3.703998
269 -3.113569 -1.861069  0.557962  0.760915
291  0.049466  0.688019 -0.292773 -3.169591
308 -0.548273  2.309776  3.038356  1.071372
321 -3.318725  0.231097 -1.211950 -1.115505
368  0.854087 -1.081998  0.812669  3.165058
420 -1.026886  0.343485  3.719493 -0.949979
540  0.588531 -0.173435 -3.312577 -0.513464
756 -3.461692 -0.864502 -0.888319 -1.677184
908 -0.026658 -0.614956  3.013013  1.303631
982  1.226694  3.517142 -0.397550 -1.219114
999  1.211094  3.081091  1.521201 -0.885738


----------排列和随机采样--------------
numpy.random.permutation函数
能够实现对series和dataframe的列的排列，通过需要排列的轴的长度调用permutation(直接传入整数n)，可以产生一个长度为n的表示新顺序的整数数组
也可以传入一个数组，返回该数组被打乱的副本
data
    0   1   2   3
0   0   1   2   3
1   4   5   6   7
2   8   9  10  11
3  12  13  14  15
4  16  17  18  19

shuffle=np.random.permutation(5)

data.iloc[shuffle]
    0   1   2   3
3  12  13  14  15
0   0   1   2   3
4  16  17  18  19
2   8   9  10  11
1   4   5   6   7

sample方法
直接选取随机子集，不用进行替换
data.sample(2)
    0   1   2   3
1   4   5   6   7
3  12  13  14  15

series=pd.Series(data=[4,2,3,1],)
series.sample(4)
1    2
0    4
3    1
2    3

replace参数
通过替换的方式产生样本（可以重复选择），若不添加该参数则无法重复选择
series.sample(8,replace=True)
0    4
0    4
3    1
2    3
3    1
2    3
0    4
0    4


-----------计算指标/哑变量------------
哑变量（Dummy Variable），又称为虚拟变量、虚设变量或名义变量，从名称上看就知道，它是人为虚设的变量，通常取值为0或1，
来反映某个变量的不同属性。对于有n个分类属性的自变量，通常需要选取1个分类作为参照，因此可以产生n-1个哑变量。

get_dummies函数
df
  key  data1
0   b      0
1   b      1
2   a      2
3   c      3
4   a      4
5   b      5

pd.get_dummies(df['key'])
   a  b  c
0  0  1  0
1  0  1  0
2  1  0  0
3  0  0  1
4  1  0  0
5  0  1  0
分析：构建哑变量，


















